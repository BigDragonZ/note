{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 高阶导数（CS/AI 专项笔记·精研版）\n",
    "## 前言\n",
    "高阶导数是一阶导数的延伸，刻画了函数变化率的**变化趋势**，是微积分中分析函数曲率、优化算法收敛性的核心工具。在AI领域，高阶导数的应用贯穿深度学习优化（如牛顿法依赖二阶导数）、激活函数平滑性评估、时序数据趋势分析等核心场景。本章将从严格定义出发，系统梳理高阶导数的数学本质、计算规则，结合AI高频案例拆解工程应用，配套代码验证与易错点辨析，形成适配Jupyter归档的结构化学习笔记。\n",
    "\n",
    "## 1. 高阶导数的严格定义与表示方法\n",
    "### 1.1 核心定义\n",
    "若函数 $y = f(x)$ 的一阶导数 $f'(x)$ 仍是关于 $x$ 的可导函数，则称 $f'(x)$ 的导数为 $f(x)$ 的**二阶导数**；同理，二阶导数的导数称为三阶导数，以此类推。\n",
    "- 一般地，函数 $f(x)$ 的 $n-1$ 阶导数的导数，称为 $f(x)$ 的**n阶导数**（$n \\geq 2$ 时，统称高阶导数）。\n",
    "- 若函数 $f(x)$ 在区间 $I$ 内 $n$ 阶可导，则称 $f(x)$ 为 $I$ 内的**n阶可导函数**，记为 $f \\in C^n(I)$（$C^n(I)$ 表示区间 $I$ 上所有n阶连续可导函数的集合）。\n",
    "\n",
    "### 1.2 常用表示方法\n",
    "高阶导数的表示需区分不同阶数，以下为单变量函数高阶导数的标准表示（以 $y = f(x)$ 为例）：\n",
    "| 导数阶数 | 拉格朗日记号 | 莱布尼茨记号 | 应用场景 |\n",
    "|----------|--------------|--------------|----------|\n",
    "| 一阶导数 | $f'(x), y'$ | $\\frac{dy}{dx}, \\frac{df(x)}{dx}$ | 梯度计算、参数更新基础 |\n",
    "| 二阶导数 | $f''(x), y''$ | $\\frac{d^2 y}{dx^2}, \\frac{d^2 f(x)}{dx^2}$ | 曲率分析、牛顿法优化 |\n",
    "| 三阶导数 | $f'''(x), y'''$ | $\\frac{d^3 y}{dx^3}$ | 高阶变化率分析（如信号加速度） |\n",
    "| n阶导数 | $f^{(n)}(x), y^{(n)}$ | $\\frac{d^n y}{dx^n}$ | 理论推导（如泰勒展开） |\n",
    "\n",
    "### 1.3 核心物理意义（直观理解）\n",
    "高阶导数的物理意义是**变化率的变化率**，以运动学为例：\n",
    "- 位移函数 $s(t)$ 的一阶导数 $s'(t) = v(t)$（瞬时速度）；\n",
    "- 二阶导数 $s''(t) = v'(t) = a(t)$（瞬时加速度）；\n",
    "- 三阶导数 $s'''(t) = a'(t)$（加速度的变化率，称为加加速度）。\n",
    "\n",
    "在AI中，损失函数 $L(\\theta)$ 的二阶导数可类比为“梯度的变化率”，反映参数更新时梯度的稳定性。\n",
    "\n",
    "## 2. 高阶导数的核心计算规则\n",
    "高阶导数的计算以一阶导数为基础，遵循“逐阶求导”的核心逻辑，同时存在针对初等函数的简化公式和运算法则，适配复杂函数的高效求解。\n",
    "\n",
    "### 2.1 基本初等函数的n阶导数公式（AI高频）\n",
    "基本初等函数的高阶导数具有固定规律，以下是AI场景中常用函数的n阶导数公式，可直接套用：\n",
    "```html\n",
    "<table style=\"width:100%; border-collapse: collapse; margin: 16px 0; font-size: 14px;\">\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #f5f5f5;\">\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">原函数 $f(x)$</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">n阶导数 $f^{(n)}(x)$</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">AI 应用场景</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">常数函数 $f(x) = C$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f^{(n)}(x) = 0$（$n \\geq 1$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">模型偏置项高阶导数计算</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">幂函数 $f(x) = x^k$（$k$ 为常数）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f^{(n)}(x) = k(k-1)\\dots(k-n+1)x^{k-n}$（$n \\leq k$）；$f^{(n)}(x)=0$（$n > k$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">多项式损失函数高阶梯度分析</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">指数函数 $f(x) = e^x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f^{(n)}(x) = e^x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">Sigmoid/Tanh函数高阶导数推导</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">正弦函数 $f(x) = \\sin x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f^{(n)}(x) = \\sin\\left(x + \\frac{n\\pi}{2}\\right)$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">振动信号高阶特征提取</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">余弦函数 $f(x) = \\cos x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f^{(n)}(x) = \\cos\\left(x + \\frac{n\\pi}{2}\\right)$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">音频信号滤波算法设计</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">对数函数 $f(x) = \\ln x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f^{(n)}(x) = (-1)^{n-1}\\frac{(n-1)!}{x^n}$（$n \\geq 1$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">交叉熵损失函数高阶导数计算</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "### 2.2 高阶导数的四则运算法则\n",
    "设函数 $u(x)$、$v(x)$ 均为n阶可导函数，$k$ 为常数，其高阶导数的四则运算遵循以下规则，核心是**莱布尼茨公式**（适配乘积运算）：\n",
    "1.  **常数倍法则**：$(k u)^{(n)} = k \\cdot u^{(n)}$\n",
    "2.  **加减法则**：$(u \\pm v)^{(n)} = u^{(n)} \\pm v^{(n)}$\n",
    "3.  **乘积法则（莱布尼茨公式）**：\n",
    "    $$(u \\cdot v)^{(n)} = \\sum_{k=0}^n \\binom{n}{k} u^{(n-k)} v^{(k)}$$\n",
    "    其中 $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ 为二项式系数，该公式是复杂乘积函数高阶导数的核心计算工具。\n",
    "\n",
    "### 2.3 复合函数的高阶导数（链式法则拓展）\n",
    "设 $y = f(u)$，$u = g(x)$，且 $f(u)$、$g(x)$ 均为高阶可导函数，则复合函数 $y = f(g(x))$ 的高阶导数需**逐层应用链式法则**，并结合乘积法则。以二阶导数为例：\n",
    "1.  一阶导数：$\\frac{dy}{dx} = f'(u) \\cdot g'(x)$\n",
    "2.  二阶导数：$\\frac{d^2 y}{dx^2} = f''(u) \\cdot [g'(x)]^2 + f'(u) \\cdot g''(x)$\n",
    "\n",
    "**AI 提示**：复合函数的高阶导数会随着阶数增加而急剧复杂，工程中通常仅用到二阶导数（如牛顿法），高阶导数（三阶及以上）较少直接应用。\n",
    "\n",
    "## 3. AI高频高阶导数计算案例（含推导过程）\n",
    "选取深度学习中激活函数、损失函数等核心场景，按“逐阶求导”逻辑拆解高阶导数推导，强化理论与工程的衔接。\n",
    "\n",
    "### 3.1 案例1：Sigmoid函数的二阶导数（优化算法收敛性分析）\n",
    "#### 函数表达式\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "#### 已知一阶导数\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "#### 二阶导数推导\n",
    "1.  展开一阶导数：$\\sigma'(x) = \\sigma(x) - \\sigma^2(x)$\n",
    "2.  逐阶求导（结合链式法则）：\n",
    "    $$\\sigma''(x) = \\sigma'(x) - 2\\sigma(x) \\cdot \\sigma'(x)$$\n",
    "3.  代入一阶导数化简：\n",
    "    $$\\sigma''(x) = \\sigma'(x)[1 - 2\\sigma(x)] = \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x))$$\n",
    "#### AI 核心价值\n",
    "二阶导数的符号反映Sigmoid函数的曲率：当 $\\sigma(x) = 0.5$（即 $x=0$）时，$\\sigma''(0) = 0.25 \\times (1 - 2 \\times 0.5) = 0$，该点为函数的拐点，是梯度变化的分界点，影响优化算法的步长选择。\n",
    "\n",
    "### 3.2 案例2：ReLU函数的高阶导数（深层网络梯度特性分析）\n",
    "#### 函数表达式\n",
    "$$\\text{ReLU}(x) = \\begin{cases} 0, & x < 0 \\\\ x, & x \\geq 0 \\end{cases}$$\n",
    "#### 逐阶导数推导\n",
    "1.  一阶导数：$\\text{ReLU}'(x) = \\begin{cases} 0, & x < 0 \\\\ 1, & x > 0 \\end{cases}$（$x=0$ 处不可导）\n",
    "2.  二阶导数：对一阶导数再次求导，$x < 0$ 或 $x > 0$ 时，$\\text{ReLU}''(x) = 0$（常数函数的导数为0）；\n",
    "3.  n阶导数（$n \\geq 2$）：$\\text{ReLU}^{(n)}(x) = 0$（$x \\neq 0$）。\n",
    "#### AI 核心价值\n",
    "ReLU函数二阶及以上导数均为0，说明其在 $x>0$ 区间是**线性的**，梯度变化率为0，可有效避免梯度震荡，这是其成为深层网络主流激活函数的重要原因之一。\n",
    "\n",
    "### 3.3 案例3：均方误差损失函数的二阶导数（牛顿法优化）\n",
    "#### 函数表达式（单样本场景）\n",
    "$$L(\\theta) = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "其中 $\\hat{y} = \\theta x + b$（线性回归预测值）。\n",
    "#### 二阶导数推导\n",
    "1.  一阶导数（对参数 $\\theta$）：$\\frac{\\partial L}{\\partial \\theta} = (\\hat{y} - y)x$\n",
    "2.  二阶导数（对参数 $\\theta$）：$\\frac{\\partial^2 L}{\\partial \\theta^2} = x^2$\n",
    "#### AI 核心价值\n",
    "均方误差的二阶导数为非负常数，说明其是**凸函数**，牛顿法可直接利用二阶导数快速收敛到最优解，无需依赖学习率调整。\n",
    "\n",
    "## 4. 高阶导数的工程实现（Python代码验证）\n",
    "针对AI高频函数，实现高阶导数的数值计算与解析验证工具，适配模型开发中的快速验证需求，避免手工推导错误。\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative_nth(f, x, n, h=1e-5):\n",
    "    \"\"\"\n",
    "    数值方法计算函数f在x处的n阶导数（中心差分法，适用于低阶导数）\n",
    "    参数：\n",
    "        f: 目标函数\n",
    "        x: 待求导的点/数组\n",
    "        n: 导数阶数\n",
    "        h: 微小增量\n",
    "    返回：\n",
    "        n阶导数的数值近似值\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return (f(x + h) - f(x - h)) / (2 * h)\n",
    "    else:\n",
    "        # 递归计算n阶导数：n阶导数是n-1阶导数的一阶导数\n",
    "        return numerical_derivative_nth(lambda t: numerical_derivative_nth(f, t, n-1, h), x, 1, h)\n",
    "\n",
    "# 定义AI高频函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv_2nd(x):\n",
    "    \"\"\"Sigmoid函数二阶导数（解析解）\"\"\"\n",
    "    sigma = sigmoid(x)\n",
    "    return sigma * (1 - sigma) * (1 - 2 * sigma)\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x >= 0, x, 0)\n",
    "\n",
    "# 验证高阶导数\n",
    "x = np.array([-1, 0, 1])\n",
    "\n",
    "# 验证Sigmoid二阶导数\n",
    "sigmoid_2nd_num = numerical_derivative_nth(sigmoid, x, 2)\n",
    "sigmoid_2nd_ana = sigmoid_deriv_2nd(x)\n",
    "print(\"Sigmoid函数二阶导数验证：\")\n",
    "print(f\"  解析解: {sigmoid_2nd_ana.round(6)}\")\n",
    "print(f\"  数值解: {sigmoid_2nd_num.round(6)}\")\n",
    "print(f\"  绝对误差: {np.abs(sigmoid_2nd_ana - sigmoid_2nd_num).round(6)}\\n\")\n",
    "\n",
    "# 验证ReLU二阶导数（x≠0处应为0）\n",
    "relu_2nd_num = numerical_derivative_nth(relu, np.array([1, -1]), 2)\n",
    "print(\"ReLU函数二阶导数验证（x=1, x=-1）：\")\n",
    "print(f\"  数值解: {relu_2nd_num.round(6)}（理论值应为0）\")\n",
    "```\n",
    "\n",
    "## 5. 高阶导数在CS/AI中的核心应用场景\n",
    "高阶导数的工程价值集中在二阶导数，其应用覆盖优化算法、激活函数评估、信号处理等多个核心领域，是提升模型性能的关键工具。\n",
    "\n",
    "### 5.1 深度学习优化算法设计\n",
    "#### 5.1.1 牛顿法与拟牛顿法\n",
    "- **核心依赖**：牛顿法的参数更新公式为 $\\theta_{t+1} = \\theta_t - [H(\\theta_t)]^{-1} \\nabla L(\\theta_t)$，其中 $H(\\theta_t)$ 是损失函数的**海森矩阵**（由二阶偏导数构成）；\n",
    "- **优势**：相比梯度下降（仅用一阶导数），牛顿法利用二阶导数刻画函数曲率，收敛速度更快，尤其适用于凸优化问题；\n",
    "- **工程妥协**：海森矩阵的计算复杂度为 $O(n^2)$（$n$ 为参数数量），大规模神经网络中难以直接计算，故常用拟牛顿法（如BFGS）近似替代。\n",
    "\n",
    "#### 5.1.2 梯度下降的学习率自适应\n",
    "- **核心逻辑**：二阶导数可评估梯度的“可信度”——二阶导数较大时，函数曲率大，梯度变化快，需减小学习率避免参数震荡；二阶导数较小时，函数平缓，可增大学习率加速收敛；\n",
    "- **典型算法**：AdaGrad、RMSProp等自适应优化算法，本质是通过梯度的二阶矩（近似二阶导数）动态调整学习率。\n",
    "\n",
    "### 5.2 激活函数的平滑性评估\n",
    "- **核心指标**：高阶导数的连续性是函数平滑性的重要衡量标准。例如：\n",
    "  - Sigmoid、Tanh函数的任意阶导数均连续，平滑性好，适合对输出精度要求高的场景；\n",
    "  - ReLU函数二阶及以上导数在非零点恒为0，但分段点处不可导，平滑性较差，可能导致模型输出存在微小突变。\n",
    "- **选型逻辑**：生成模型（如GAN）对函数平滑性要求高，优先选择高阶可导的激活函数（如Mish、Swish）；而分类任务中，ReLU的低计算复杂度优势更突出。\n",
    "\n",
    "### 5.3 信号处理与时序数据分析\n",
    "- **振动信号特征提取**：加速度（位移的二阶导数）、加加速度（位移的三阶导数）是设备故障诊断的重要特征，可通过高阶导数捕捉信号的突变信息；\n",
    "- **音频信号处理**：声音的频率变化可通过声波函数的高阶导数刻画，用于语音识别中的情感分析、语调检测。\n",
    "\n",
    "### 5.4 泰勒展开与函数近似\n",
    "- **核心应用**：高阶导数是泰勒展开的核心组成部分，函数 $f(x)$ 在 $x_0$ 处的泰勒展开式为：\n",
    "  $$f(x) = \\sum_{n=0}^\\infty \\frac{f^{(n)}(x_0)}{n!}(x - x_0)^n$$\n",
    "- **工程价值**：深度学习中，复杂函数可通过低阶泰勒展开近似简化计算，例如损失函数在最优解附近的二阶泰勒近似，可快速分析模型的局部收敛特性。\n",
    "\n",
    "## 6. 常见误区与易错点辨析\n",
    "```html\n",
    "<table style=\"width:100%; border-collapse: collapse; margin: 16px 0; font-size: 14px;\">\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #f5f5f5;\">\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">易错点</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">错误认知</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">正确结论</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">AI 避坑措施</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">复合函数高阶导数漏项</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$\\frac{d^2 y}{dx^2} = f''(u) \\cdot g''(x)$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">二阶导数含两项：$f''(u) \\cdot [g'(x)]^2 + f'(u) \\cdot g''(x)$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">推导时分步书写，先求一阶导数，再对一阶导数逐因子求导</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">认为高阶可导必为光滑函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">函数n阶可导，则一定是光滑函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">光滑函数需任意阶可导，n阶可导仅保证前n阶导数存在，高阶可能不可导</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">激活函数选型时，验证其任意阶导数的连续性（如Sigmoid满足，ReLU不满足）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">海森矩阵与二阶导数混淆</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">多变量函数的二阶导数就是海森矩阵</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">海森矩阵是二阶偏导数的矩阵形式，包含所有变量的交叉偏导数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">牛顿法中明确区分梯度（一阶偏导数向量）与海森矩阵（二阶偏导数矩阵）</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">忽略分段函数的高阶导数不可导点</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">ReLU函数的二阶导数在x=0处为0</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">ReLU在x=0处一阶导数不存在，故高阶导数更不存在，仅非零点二阶导数为0</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">训练中通过梯度裁剪避免参数落入分段点，减少不可导点对高阶优化的影响</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "## 7. 拓展联系与学习建议\n",
    "### 7.1 与后续知识的衔接\n",
    "1.  **偏导数与海森矩阵**：多变量函数的高阶导数核心是**二阶偏导数**，其矩阵化形式为海森矩阵，是多元优化的基础；\n",
    "2.  **微分方程**：高阶线性微分方程的求解依赖高阶导数，在时序预测、强化学习的动态系统建模中有重要应用；\n",
    "3.  **泛函分析**：高阶导数可推广到函数空间，形成“泛函导数”，是深度学习中变分推断、神经网络梯度计算的理论基础。\n",
    "\n",
    "### 7.2 学习建议（CS/AI 方向专属）\n",
    "1.  **聚焦二阶导数核心**：AI工程中高阶导数的应用以二阶为主，重点掌握二阶导数的计算规则、物理意义及在优化算法中的应用，无需过度深究三阶及以上导数；\n",
    "2.  **绑定优化算法学习**：将二阶导数与牛顿法、海森矩阵、自适应学习率等算法绑定学习，理解“二阶导数→曲率→收敛速度”的逻辑链，避免孤立记忆；\n",
    "3.  **强化代码验证能力**：使用数值差分法验证解析导数的正确性，尤其是复合函数和分段函数的二阶导数，直观感受高阶导数的数值特性；\n",
    "4.  **结合框架源码理解**：阅读PyTorch、TensorFlow中优化器（如`torch.optim.LBFGS`）的源码，了解高阶导数在工程中的近似实现方法，深化理论与实践的衔接。\n",
    "\n",
    "是否需要我针对**海森矩阵的构建与应用**或**牛顿法优化神经网络的完整代码实现**，提供更详细的案例推导和工程实践方案？"
   ],
   "id": "ae043eae0f6d09ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
