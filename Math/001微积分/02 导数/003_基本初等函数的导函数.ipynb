{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 基本初等函数的导函数（CS/AI 专项笔记·精研版）\n",
    "## 前言\n",
    "基本初等函数的导函数是微积分运算的**基础工具库**，更是AI领域中激活函数梯度计算、损失函数求导、优化算法参数更新的核心数学支撑。基本初等函数包括常数函数、幂函数、指数函数、对数函数、三角函数、反三角函数六大类，其导函数具有固定公式和清晰的推导逻辑。本章将系统梳理每类函数的导函数公式、严格数学推导、核心推论，并紧密结合CS/AI工程场景，解析其在深度学习、数值计算中的实战应用，同时配套易错点辨析和代码验证，形成“公式-推导-应用-验证”的完整知识闭环。\n",
    "\n",
    "## 1. 基本初等函数导函数总表（CS/AI高频版）\n",
    "以下表格汇总了六大类基本初等函数的原函数、导函数公式，并标注了AI中的核心应用场景，便于快速查阅和工程调用。\n",
    "```html\n",
    "<table style=\"width:100%; border-collapse: collapse; margin: 16px 0; font-size: 14px;\">\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #f5f5f5;\">\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">函数类别</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">原函数 $f(x)$</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">导函数 $f'(x)$</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">AI 核心应用场景</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd; vertical-align: top;\" rowspan=\"1\">常数函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = C$（$C$ 为常数）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = 0$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">模型偏置项求导、固定超参数梯度计算</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd; vertical-align: top;\" rowspan=\"1\">幂函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = x^k$（$k$ 为常数）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = kx^{k-1}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">多项式损失函数梯度、学习率衰减函数求导</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd; vertical-align: top;\" rowspan=\"2\">指数函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = e^x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = e^x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">Sigmoid/Tanh激活函数梯度计算核心</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = a^x$（$a>0,a≠1$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = a^x \\ln a$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">特殊场景衰减函数（如指数学习率）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd; vertical-align: top;\" rowspan=\"2\">对数函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\ln x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = \\frac{1}{x}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">交叉熵损失函数求导、信息增益计算</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\log_a x$（$a>0,a≠1$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = \\frac{1}{x \\ln a}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">传统机器学习决策树特征选择</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd; vertical-align: top;\" rowspan=\"4\">三角函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\sin x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = \\cos x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">图像旋转、振动信号特征提取</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\cos x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = -\\sin x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">音频信号滤波、周期性数据建模</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\tan x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = \\sec^2 x = \\frac{1}{\\cos^2 x}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">特殊激活函数设计（如周期性激活）</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\cot x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = -\\csc^2 x = -\\frac{1}{\\sin^2 x}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">工程信号相位计算</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd; vertical-align: top;\" rowspan=\"4\">反三角函数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\arcsin x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = \\frac{1}{\\sqrt{1 - x^2}}$（$|x|<1$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">数据归一化逆变换、角度映射</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\arccos x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = -\\frac{1}{\\sqrt{1 - x^2}}$（$|x|<1$）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">空间几何角度计算</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\arctan x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = \\frac{1}{1 + x^2}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">平滑激活函数设计、梯度裁剪</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f(x) = \\text{arccot } x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$f'(x) = -\\frac{1}{1 + x^2}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">概率分布转换</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "## 2. 核心导函数公式的严格推导（AI高频重点）\n",
    "针对AI中应用最广泛的几类基本初等函数，从导数的极限定义出发，进行严格数学推导，揭示公式的来源，避免机械记忆。\n",
    "\n",
    "### 2.1 幂函数：$f(x) = x^k$（$k$ 为常数）\n",
    "#### 导函数公式\n",
    "$$f'(x) = kx^{k-1}$$\n",
    "\n",
    "#### 推导过程（分实数域通用推导）\n",
    "1.  **导数定义切入**：由导数的增量形式，$f'(x) = \\lim_{\\Delta x \\to 0} \\frac{(x + \\Delta x)^k - x^k}{\\Delta x}$；\n",
    "2.  **二项式展开（整数 $k$ 特殊情况）**：当 $k$ 为正整数时，$(x + \\Delta x)^k = x^k + kx^{k-1}\\Delta x + o(\\Delta x)$，代入得：\n",
    "    $$\\frac{(x + \\Delta x)^k - x^k}{\\Delta x} = \\frac{kx^{k-1}\\Delta x + o(\\Delta x)}{\\Delta x} = kx^{k-1} + \\frac{o(\\Delta x)}{\\Delta x}$$\n",
    "    当 $\\Delta x \\to 0$ 时，$\\frac{o(\\Delta x)}{\\Delta x} \\to 0$，故 $f'(x) = kx^{k-1}$；\n",
    "3.  **实数 $k$ 推广**：对任意实数 $k$，利用指数对数转换 $x^k = e^{k \\ln x}$，结合复合函数求导法则推导，结果一致。\n",
    "\n",
    "#### 核心推论（AI高频）\n",
    "- 当 $k = 1$ 时，$f(x) = x$，$f'(x) = 1$（线性函数导数为常数，线性回归中权重梯度计算基础）；\n",
    "- 当 $k = -1$ 时，$f(x) = \\frac{1}{x}$，$f'(x) = -\\frac{1}{x^2}$（分式函数求导，损失函数正则项计算）；\n",
    "- 当 $k = \\frac{1}{2}$ 时，$f(x) = \\sqrt{x}$，$f'(x) = \\frac{1}{2\\sqrt{x}}$（根号函数求导，特征归一化场景）。\n",
    "\n",
    "### 2.2 指数函数：$f(x) = e^x$\n",
    "#### 导函数公式\n",
    "$$f'(x) = e^x$$\n",
    "\n",
    "#### 推导过程\n",
    "1.  **导数定义展开**：$f'(x) = \\lim_{\\Delta x \\to 0} \\frac{e^{x + \\Delta x} - e^x}{\\Delta x} = e^x \\cdot \\lim_{\\Delta x \\to 0} \\frac{e^{\\Delta x} - 1}{\\Delta x}$；\n",
    "2.  **重要极限代入**：由重要极限 $\\lim_{t \\to 0} \\frac{e^t - 1}{t} = 1$（令 $t = \\Delta x$），得 $\\lim_{\\Delta x \\to 0} \\frac{e^{\\Delta x} - 1}{\\Delta x} = 1$；\n",
    "3.  **结论**：$f'(x) = e^x \\cdot 1 = e^x$。\n",
    "\n",
    "#### AI 核心价值\n",
    "$e^x$ 的导数等于自身，这一独特性质使Sigmoid、Tanh等激活函数的梯度计算大幅简化，是深度学习中梯度传播的关键简化依据。\n",
    "\n",
    "### 2.3 对数函数：$f(x) = \\ln x$\n",
    "#### 导函数公式\n",
    "$$f'(x) = \\frac{1}{x}$$\n",
    "\n",
    "#### 推导过程\n",
    "1.  **导数定义展开**：$f'(x) = \\lim_{\\Delta x \\to 0} \\frac{\\ln(x + \\Delta x) - \\ln x}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac{\\ln\\left(1 + \\frac{\\Delta x}{x}\\right)}{\\Delta x}$；\n",
    "2.  **变量替换**：令 $t = \\frac{\\Delta x}{x}$，则 $\\Delta x = xt$，当 $\\Delta x \\to 0$ 时 $t \\to 0$，代入得：\n",
    "    $$f'(x) = \\lim_{t \\to 0} \\frac{\\ln(1 + t)}{xt} = \\frac{1}{x} \\cdot \\lim_{t \\to 0} \\frac{\\ln(1 + t)}{t}$$\n",
    "3.  **重要极限代入**：由重要极限 $\\lim_{t \\to 0} \\frac{\\ln(1 + t)}{t} = 1$，得 $f'(x) = \\frac{1}{x} \\cdot 1 = \\frac{1}{x}$。\n",
    "\n",
    "#### AI 核心价值\n",
    "交叉熵损失函数 $L = -\\sum y \\ln \\hat{y}$ 的梯度计算，直接依赖 $\\ln x$ 的导数公式，避免了复杂的对数求导运算。\n",
    "\n",
    "### 2.4 三角函数：$f(x) = \\sin x$\n",
    "#### 导函数公式\n",
    "$$f'(x) = \\cos x$$\n",
    "\n",
    "#### 推导过程\n",
    "1.  **导数定义展开**：$f'(x) = \\lim_{\\Delta x \\to 0} \\frac{\\sin(x + \\Delta x) - \\sin x}{\\Delta x}$；\n",
    "2.  **三角恒等式拆分**：利用和角公式 $\\sin(A + B) = \\sin A \\cos B + \\cos A \\sin B$，得：\n",
    "    $$\\sin(x + \\Delta x) - \\sin x = \\sin x (\\cos \\Delta x - 1) + \\cos x \\sin \\Delta x$$\n",
    "    代入导数定义式：\n",
    "    $$f'(x) = \\sin x \\cdot \\lim_{\\Delta x \\to 0} \\frac{\\cos \\Delta x - 1}{\\Delta x} + \\cos x \\cdot \\lim_{\\Delta x \\to 0} \\frac{\\sin \\Delta x}{\\Delta x}$$\n",
    "3.  **重要极限代入**：由 $\\lim_{\\Delta x \\to 0} \\frac{\\sin \\Delta x}{\\Delta x} = 1$ 和 $\\lim_{\\Delta x \\to 0} \\frac{\\cos \\Delta x - 1}{\\Delta x} = 0$，得：\n",
    "    $$f'(x) = \\sin x \\cdot 0 + \\cos x \\cdot 1 = \\cos x$$\n",
    "\n",
    "#### AI 核心价值\n",
    "图像旋转、振动信号处理中，$\\sin x$ 和其导数 $\\cos x$ 常成对出现，用于描述旋转角度、振动频率等物理量的变化率。\n",
    "\n",
    "### 2.5 反三角函数：$f(x) = \\arctan x$\n",
    "#### 导函数公式\n",
    "$$f'(x) = \\frac{1}{1 + x^2}$$\n",
    "\n",
    "#### 推导过程\n",
    "1.  **隐函数求导法**：令 $y = \\arctan x$，则 $x = \\tan y$（$y \\in \\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)$）；\n",
    "2.  **两边对 $x$ 求导**：左边导数为 $1$，右边由复合函数求导得 $\\frac{d}{dx}(\\tan y) = \\sec^2 y \\cdot \\frac{dy}{dx}$；\n",
    "3.  **化简求解**：由三角恒等式 $\\sec^2 y = 1 + \\tan^2 y = 1 + x^2$，得 $\\frac{dy}{dx} = \\frac{1}{\\sec^2 y} = \\frac{1}{1 + x^2}$。\n",
    "\n",
    "#### AI 核心价值\n",
    "$\\arctan x$ 的导数取值范围为 $(0,1]$，具有良好的梯度稳定性，常被用于设计平滑激活函数，避免梯度消失或爆炸。\n",
    "\n",
    "## 3. 导函数的核心推论与AI工程应用\n",
    "基于基本初等函数的导函数，可推导得出一系列常用推论，这些推论是AI中复杂函数求导的直接工具，覆盖激活函数、损失函数、优化算法等核心场景。\n",
    "\n",
    "### 3.1 常用推论（简化计算工具）\n",
    "1.  **负指数函数导数**：$\\left(e^{-x}\\right)' = -e^{-x}$（Sigmoid函数梯度计算的核心推论）；\n",
    "2.  **复合幂函数导数**：$\\left((ax + b)^k\\right)' = ka(ax + b)^{k-1}$（多层神经网络中线性层求导）；\n",
    "3.  **三角函数复合导数**：$(\\sin(ax))' = a\\cos(ax)$，$(\\cos(ax))' = -a\\sin(ax)$（信号处理中频率缩放场景）；\n",
    "4.  **对数复合导数**：$(\\ln(ax + b))' = \\frac{a}{ax + b}$（复杂损失函数中的对数项求导）。\n",
    "\n",
    "### 3.2 典型AI工程应用案例\n",
    "#### 案例1：Sigmoid激活函数梯度计算\n",
    "Sigmoid函数 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$，其梯度推导依赖指数函数和幂函数的导数推论：\n",
    "$$\\sigma'(x) = -\\frac{1}{(1 + e^{-x})^2} \\cdot (-e^{-x}) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "该结果大幅简化了梯度计算，是二分类模型反向传播的基础。\n",
    "\n",
    "#### 案例2：均方误差损失函数梯度计算\n",
    "均方误差损失 $L = \\frac{1}{2}(y - \\hat{y})^2$，其中 $\\hat{y} = wx + b$，其对权重 $w$ 的梯度推导依赖幂函数和复合函数导数：\n",
    "$$\\frac{\\partial L}{\\partial w} = (y - \\hat{y}) \\cdot (-\\hat{y}') = (y - \\hat{y}) \\cdot (-x) = x(\\hat{y} - y)$$\n",
    "该公式是线性回归、神经网络全连接层参数更新的核心依据。\n",
    "\n",
    "#### 案例3：Tanh激活函数梯度计算\n",
    "Tanh函数 $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$，其梯度推导依赖指数函数导数：\n",
    "$$\\tanh'(x) = \\frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2} = 1 - \\tanh^2(x)$$\n",
    "梯度取值范围为 $(0,1]$，稳定性优于Sigmoid函数，适用于深层网络。\n",
    "\n",
    "## 4. 工程实现（Python 导函数验证工具）\n",
    "通过Python代码实现基本初等函数的导函数计算，并采用数值差分法验证解析导数的正确性，确保工程应用中的准确性。\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-6):\n",
    "    \"\"\"\n",
    "    有限差分法计算数值导数（用于验证解析导数）\n",
    "    参数：\n",
    "        f: 原函数\n",
    "        x: 待求导的点/数组\n",
    "        h: 微小增量\n",
    "    返回：\n",
    "        数值导数\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# 定义基本初等函数及其解析导函数\n",
    "functions = [\n",
    "    # (原函数, 解析导函数, 测试点, 函数名称)\n",
    "    (lambda x: np.ones_like(x) * 5, lambda x: np.zeros_like(x), 2, \"常数函数 f(x)=5\"),\n",
    "    (lambda x: x ** 3, lambda x: 3 * x ** 2, 2, \"幂函数 f(x)=x³\"),\n",
    "    (lambda x: np.exp(x), lambda x: np.exp(x), 0, \"指数函数 f(x)=eˣ\"),\n",
    "    (lambda x: np.log(x), lambda x: 1 / x, 1, \"对数函数 f(x)=lnx\"),\n",
    "    (lambda x: np.sin(x), lambda x: np.cos(x), 0, \"三角函数 f(x)=sinx\"),\n",
    "    (lambda x: np.arctan(x), lambda x: 1 / (1 + x ** 2), 1, \"反三角函数 f(x)=arctanx\")\n",
    "]\n",
    "\n",
    "# 验证并输出结果\n",
    "for f, f_deriv_analytic, x0, name in functions:\n",
    "    # 计算解析导数和数值导数\n",
    "    deriv_analytic = f_deriv_analytic(x0)\n",
    "    deriv_numerical = numerical_derivative(f, x0)\n",
    "    # 计算绝对误差\n",
    "    error = np.abs(deriv_analytic - deriv_numerical)\n",
    "    # 输出结果\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  解析导数: {deriv_analytic:.6f}\")\n",
    "    print(f\"  数值导数: {deriv_numerical:.6f}\")\n",
    "    print(f\"  绝对误差: {error:.6e}\\n\")\n",
    "\n",
    "# AI专项：激活函数梯度计算验证\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    sigma = sigmoid(x)\n",
    "    return sigma * (1 - sigma)\n",
    "\n",
    "x = np.array([-1, 0, 1])\n",
    "sigmoid_grad_analytic = sigmoid_deriv(x)\n",
    "sigmoid_grad_numerical = numerical_derivative(sigmoid, x)\n",
    "print(\"Sigmoid激活函数梯度验证：\")\n",
    "print(f\"  解析梯度: {sigmoid_grad_analytic.round(6)}\")\n",
    "print(f\"  数值梯度: {sigmoid_grad_numerical.round(6)}\")\n",
    "```\n",
    "\n",
    "## 5. 常见误区与易错点辨析\n",
    "```html\n",
    "<table style=\"width:100%; border-collapse: collapse; margin: 16px 0; font-size: 14px;\">\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #f5f5f5;\">\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">易错点</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">错误认知</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">正确结论</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">AI 避坑措施</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">常数函数导数误记</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$C' = C$（$C$ 为常数）</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">常数函数导数为0，即 $C' = 0$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">模型偏置项求导时，直接取0，避免额外计算</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">幂函数导数漏乘指数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(x^k)' = x^{k-1}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">需乘指数 $k$，即 $(x^k)' = kx^{k-1}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">多项式损失函数求导后，检查是否含指数因子</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">对数函数导数底数混淆</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(\\log_a x)' = \\frac{1}{x}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">自然对数导数为 $\\frac{1}{x}$，一般对数需除以 $\\ln a$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">交叉熵损失函数中仅用自然对数，避免使用一般对数</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">三角函数导数符号错误</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(\\cos x)' = \\sin x$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$\\cos x$ 的导数为 $-\\sin x$，符号不可遗漏</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">信号处理中，导数符号影响相位判断，需严格核对</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">反三角函数定义域忽略</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(\\arcsin x)'$ 对所有 $x$ 成立</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">仅当 $|x| < 1$ 时成立，$|x|=1$ 时导数不存在</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">使用反三角函数前，确保输入值在定义域内</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "## 6. 学习建议（CS/AI 方向专属）\n",
    "1.  **核心公式优先掌握**：重点牢记指数函数（$e^x$）、对数函数（$\\ln x$）、幂函数（$x^k$）和三角函数（$\\sin x, \\cos x$）的导函数公式，它们是AI中激活函数、损失函数求导的基础；\n",
    "2.  **推导逻辑重于记忆**：理解每个公式的推导过程（如利用重要极限、三角恒等式、隐函数求导），避免机械记忆，遇到复合函数时可灵活拆解；\n",
    "3.  **工程验证强化理解**：通过Python数值差分法验证解析导数的正确性，直观感受导数的数值特性，同时为后续复杂函数求导排查错误；\n",
    "4.  **关联AI场景深化应用**：每掌握一个导函数公式，立即关联对应的AI场景（如 $\\ln x$ 对应交叉熵损失），构建“公式-场景-代码”的闭环记忆，提升知识复用能力。\n",
    "\n",
    "是否需要我针对**复合激活函数的导函数推导**或**深度学习反向传播中的导函数链式应用**，提供更详细的案例和代码实现？"
   ],
   "id": "a9bc4af8d2400bb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
