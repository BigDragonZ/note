{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 微积分核心笔记（AI/CS 方向）：反函数和复合函数求导\n",
    "## 一、精确定义\n",
    "### 1. 反函数求导定义\n",
    "设函数 \\( y = f(x) \\) 在区间 \\( I_x \\) 内**单调、可导**，且 \\( f'(x) \\neq 0 \\)，则其反函数 \\( x = f^{-1}(y) \\)（记为 \\( x = \\varphi(y) \\)，其中 \\( \\varphi = f^{-1} \\)）在对应区间 \\( I_y = \\{ y \\mid y = f(x), x \\in I_x \\} \\) 内也可导，且反函数的导数与原函数的导数满足：\n",
    "\\[\n",
    "\\boxed{(\\varphi(y))' = \\frac{1}{f'(\\varphi(y))} \\quad \\text{或等价表示为} \\quad \\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}}}\n",
    "\\]\n",
    "**核心说明**：反函数的导数是原函数导数的倒数，但需注意“变量对齐”——反函数的导数是对 \\( y \\) 求导，原函数的导数是对 \\( x \\) 求导，最终结果需用 \\( y \\) 或 \\( x \\) 统一变量（AI 中常转化为对输入变量 \\( x \\) 的导数）。\n",
    "\n",
    "### 2. 复合函数求导定义（链式法则）\n",
    "设函数 \\( u = g(x) \\) 在点 \\( x \\) 处可导，函数 \\( y = f(u) \\) 在对应点 \\( u = g(x) \\) 处可导，则复合函数 \\( y = f(g(x)) \\)（记为 \\( y = f \\circ g(x) \\)）在点 \\( x \\) 处可导，且其导数为：\n",
    "\\[\n",
    "\\boxed{\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} \\quad \\text{或简写为} \\quad y'_x = y'_u \\cdot u'_x}\n",
    "\\]\n",
    "**推广形式**：有限个函数复合的链式法则\n",
    "若 \\( y = f(u), u = g(v), v = h(x) \\) 均为可导函数，则：\n",
    "\\[\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dv} \\cdot \\frac{dv}{dx}\n",
    "\\]\n",
    "**核心说明**：复合函数的导数等于“外层函数对中间变量的导数”乘以“中间变量对自变量的导数”，链式法则的本质是“变化率的传递”（AI 中反向传播的核心原理）。\n",
    "\n",
    "\n",
    "## 二、公式推导\n",
    "### 1. 反函数求导推导（基于导数定义）\n",
    "要证明 \\( (\\varphi(y))' = \\frac{1}{f'(\\varphi(y))} \\)，步骤如下：\n",
    "1. **反函数的单调性与连续性**：因 \\( y = f(x) \\) 在 \\( I_x \\) 内单调可导，故 \\( f(x) \\) 单调连续，其反函数 \\( x = \\varphi(y) \\) 也单调连续（微积分基本性质）；\n",
    "2. **增量关系**：对 \\( y = f(x) \\)，当 \\( \\Delta y \\neq 0 \\) 时，由单调性可知 \\( \\Delta x = \\varphi(y + \\Delta y) - \\varphi(y) \\neq 0 \\)，且 \\( \\Delta y \\to 0 \\iff \\Delta x \\to 0 \\)（连续性）；\n",
    "3. **导数定义展开**：\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\varphi'(y) &= \\lim_{\\Delta y \\to 0} \\frac{\\Delta x}{\\Delta y} = \\lim_{\\Delta y \\to 0} \\frac{1}{\\frac{\\Delta y}{\\Delta x}} \\\\\n",
    "&= \\lim_{\\Delta x \\to 0} \\frac{1}{\\frac{\\Delta y}{\\Delta x}} \\quad (\\text{因} \\Delta y \\to 0 \\iff \\Delta x \\to 0) \\\\\n",
    "&= \\frac{1}{\\lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x}} = \\frac{1}{f'(x)}\n",
    "\\end{align*}\n",
    "\\]\n",
    "4. **变量替换**：因 \\( x = \\varphi(y) \\)，故 \\( f'(x) = f'(\\varphi(y)) \\)，最终得：\n",
    "\\[\n",
    "\\varphi'(y) = \\frac{1}{f'(\\varphi(y))}\n",
    "\\]\n",
    "\n",
    "### 2. 复合函数求导推导（链式法则）\n",
    "要证明 \\( \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} \\)，步骤如下：\n",
    "1. **增量分解**：设自变量 \\( x \\) 有增量 \\( \\Delta x \\)，则中间变量 \\( u = g(x) \\) 产生增量 \\( \\Delta u = g(x + \\Delta x) - g(x) \\)，进而导致因变量 \\( y = f(u) \\) 产生增量 \\( \\Delta y = f(u + \\Delta u) - f(u) \\)；\n",
    "2. **可导性与连续性**：因 \\( y = f(u) \\) 在 \\( u \\) 处可导，故 \\( \\Delta y = f'(u) \\Delta u + o(\\Delta u) \\)（微分的定义，其中 \\( o(\\Delta u) \\) 是 \\( \\Delta u \\) 的高阶无穷小，即 \\( \\lim_{\\Delta u \\to 0} \\frac{o(\\Delta u)}{\\Delta u} = 0 \\)）；\n",
    "3. **增量比转化**：两边除以 \\( \\Delta x \\)（\\( \\Delta x \\neq 0 \\)）：\n",
    "\\[\n",
    "\\frac{\\Delta y}{\\Delta x} = f'(u) \\cdot \\frac{\\Delta u}{\\Delta x} + \\frac{o(\\Delta u)}{\\Delta x}\n",
    "\\]\n",
    "4. **取极限**：因 \\( u = g(x) \\) 在 \\( x \\) 处可导，故 \\( \\lim_{\\Delta x \\to 0} \\frac{\\Delta u}{\\Delta x} = u'(x) \\)，且 \\( u \\) 连续（可导必连续），故 \\( \\lim_{\\Delta x \\to 0} \\Delta u = 0 \\)，进而：\n",
    "\\[\n",
    "\\lim_{\\Delta x \\to 0} \\frac{o(\\Delta u)}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\left( \\frac{o(\\Delta u)}{\\Delta u} \\cdot \\frac{\\Delta u}{\\Delta x} \\right) = 0 \\cdot u'(x) = 0\n",
    "\\]\n",
    "5. **最终结果**：对增量比取极限：\n",
    "\\[\n",
    "\\frac{dy}{dx} = \\lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x} = f'(u) \\cdot u'(x) = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "\\]\n",
    "\n",
    "\n",
    "## 三、原理分析\n",
    "### 1. 反函数求导的核心本质\n",
    "- **几何意义**：原函数 \\( y = f(x) \\) 与反函数 \\( x = \\varphi(y) \\) 的图像关于直线 \\( y = x \\) 对称，因此原函数在点 \\( (x_0, y_0) \\) 处的切线斜率 \\( f'(x_0) \\) 与反函数在点 \\( (y_0, x_0) \\) 处的切线斜率 \\( \\varphi'(y_0) \\) 互为倒数（斜率乘积为 1）；\n",
    "- **关键约束**：必须满足 \\( f'(x) \\neq 0 \\)（否则反函数在对应点不可导），且原函数需单调（保证反函数存在且唯一）——AI 中常用的单调函数（如 Sigmoid 的反函数 Logit、Softmax 的反函数 Log-Softmax）均满足此条件；\n",
    "- **变量转换技巧**：推导结果 \\( \\varphi'(y) = \\frac{1}{f'(x)} \\) 中，\\( x = \\varphi(y) \\)，实际应用中需将导数表达式转化为关于 \\( y \\) 的函数（或再转换为关于 \\( x \\) 的函数），避免变量混淆。\n",
    "\n",
    "### 2. 复合函数求导的核心本质\n",
    "- **变化率传递**：复合函数的变化率是“各层变化率的乘积”——例如 \\( y = f(g(x)) \\) 中，\\( x \\) 变化导致 \\( u = g(x) \\) 变化（变化率 \\( u'_x \\)），\\( u \\) 变化再导致 \\( y \\) 变化（变化率 \\( y'_u \\)），最终 \\( y \\) 对 \\( x \\) 的变化率是两者的乘积；\n",
    "- **“链式”不可断**：多函数复合时，需按“从外到内”或“从内到外”的顺序依次求导，不可遗漏中间变量的导数——例如 \\( y = f(g(h(x))) \\) 需先求 \\( f' \\) 再求 \\( g' \\) 最后求 \\( h' \\)，再相乘；\n",
    "- **与四则运算的兼容**：AI 中函数多为“复合+四则运算”的组合（如 \\( y = x \\cdot \\sigma(Wx + b) \\)），需同时应用链式法则和四则运算，优先级为：先分解复合结构，再处理四则运算。\n",
    "\n",
    "### 3. 基础例题（含 AI 常用函数）\n",
    "#### （1）反函数求导例题\n",
    "| 原函数 \\( y = f(x) \\) | 反函数 \\( x = \\varphi(y) \\) | 原函数导数 \\( f'(x) \\) | 反函数导数 \\( \\varphi'(y) \\) | 应用场景 |\n",
    "|-----------------------|-----------------------------|------------------------|------------------------------|----------|\n",
    "| \\( y = e^x \\)（指数函数） | \\( x = \\ln y \\)（对数函数） | \\( f'(x) = e^x \\) | \\( \\varphi'(y) = \\frac{1}{e^{\\ln y}} = \\frac{1}{y} \\)（即 \\( (\\ln x)' = \\frac{1}{x} \\)） | 对数损失函数求导 |\n",
    "| \\( y = \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)（Sigmoid） | \\( x = \\text{Logit}(y) = \\ln \\frac{y}{1 - y} \\)（Logit 函数） | \\( \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) \\) | \\( \\text{Logit}'(y) = \\frac{1}{y(1 - y)} \\) | 分类模型概率与对数几率转换 |\n",
    "| \\( y = \\arctan x \\)（反正切函数） | \\( x = \\tan y \\)（正切函数） | \\( f'(x) = \\frac{1}{1 + x^2} \\) | \\( \\varphi'(y) = 1 + \\tan^2 y = 1 + x^2 \\)（即 \\( (\\arctan x)' = \\frac{1}{1 + x^2} \\)） | 激活函数求导、梯度裁剪 |\n",
    "\n",
    "#### （2）复合函数求导例题\n",
    "| 复合函数 \\( y = f(g(x)) \\) | 分解（\\( y = f(u), u = g(x) \\)） | 导数计算（链式法则） | 应用场景 |\n",
    "|----------------------------|----------------------------------|----------------------|----------|\n",
    "| \\( y = \\sin(x^2 + e^x) \\) | \\( f(u) = \\sin u, u = x^2 + e^x \\) | \\( y' = \\cos u \\cdot (2x + e^x) = \\cos(x^2 + e^x) \\cdot (2x + e^x) \\) | 周期性信号处理 |\n",
    "| \\( y = \\sigma(Wx + b) \\)（Sigmoid 激活） | \\( f(u) = \\frac{1}{1 + e^{-u}}, u = Wx + b \\) | \\( y' = \\sigma(u)(1 - \\sigma(u)) \\cdot W = \\sigma(Wx + b)(1 - \\sigma(Wx + b)) \\cdot W \\) | 神经网络层梯度计算 |\n",
    "| \\( y = (x^3 + 2x) \\cdot \\cos(e^x) \\)（复合+乘积） | \\( f(u) = (x^3 + 2x) \\cdot \\cos u, u = e^x \\) | \\( y' = 3x^2 + 2) \\cos u + (x^3 + 2x) \\cdot (-\\sin u) \\cdot e^x = (3x^2 + 2)\\cos(e^x) - e^x(x^3 + 2x)\\sin(e^x) \\) | 复杂损失函数求导 |\n",
    "| \\( y = \\text{ReLU}(Wx + b) \\)（ReLU 激活） | \\( f(u) = \\max(0, u), u = Wx + b \\) | \\( y' = \\begin{cases} W & u > 0 \\\\ 0 & u < 0 \\end{cases} \\)（不可导点 \\( u=0 \\) 通常取 0 或 1） | 深度学习常用激活函数梯度 |\n",
    "\n",
    "\n",
    "## 四、CS/AI 应用场景\n",
    "### 1. 反函数求导的核心应用\n",
    "#### （1）Logit 函数与 Sigmoid 函数的梯度转换\n",
    "二分类模型中，Sigmoid 函数 \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\) 将对数几率 \\( x = \\ln \\frac{p}{1 - p} \\)（Logit 函数）映射为概率 \\( p \\)，其反函数 Logit 函数的导数是模型参数更新的关键：\n",
    "- Logit 函数：\\( \\text{Logit}(p) = \\ln \\frac{p}{1 - p} = \\ln p - \\ln(1 - p) \\)\n",
    "- 导数计算（应用反函数求导）：\\( \\text{Logit}'(p) = \\frac{1}{p} + \\frac{1}{1 - p} = \\frac{1}{p(1 - p)} \\)\n",
    "- 应用场景：在逻辑回归中，损失函数对概率 \\( p \\) 的梯度可通过 Logit 导数转换为对参数 \\( \\theta \\) 的梯度（\\( x = \\theta^T \\mathbf{x} \\)）。\n",
    "\n",
    "#### （2）生成模型中的逆映射梯度\n",
    "生成对抗网络（GAN）、变分自编码器（VAE）等生成模型中，常需通过“原分布→ latent 分布”的映射及其逆映射计算梯度：\n",
    "- 示例：若生成器 \\( G(z) \\) 将 latent 变量 \\( z \\) 映射为数据 \\( x = G(z) \\)，则判别器或后验分布的梯度计算需用到逆映射 \\( z = G^{-1}(x) \\) 的导数，即 \\( (G^{-1})'(x) = \\frac{1}{G'(z)} \\)。\n",
    "\n",
    "### 2. 复合函数求导（链式法则）的核心应用\n",
    "#### （1）神经网络反向传播（BP 算法）的核心\n",
    "反向传播的本质是**链式法则的逐层应用**，以全连接神经网络为例（输入 \\( \\mathbf{x} \\)，隐藏层 \\( \\mathbf{h} = \\sigma(W_1 \\mathbf{x} + b_1) \\)，输出 \\( \\hat{\\mathbf{y}} = \\sigma(W_2 \\mathbf{h} + b_2) \\)，损失 \\( L(\\hat{\\mathbf{y}}, \\mathbf{y}) \\)）：\n",
    "1. 输出层梯度（对输出层权重 \\( W_2 \\)）：\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} \\cdot \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{z}_2} \\cdot \\mathbf{h}^T \\quad (\\mathbf{z}_2 = W_2 \\mathbf{h} + b_2)\n",
    "\\]\n",
    "   - 其中 \\( \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{z}_2} = \\sigma'(\\mathbf{z}_2) \\)（激活函数导数），体现链式法则的“层间传递”；\n",
    "2. 隐藏层梯度（对隐藏层权重 \\( W_1 \\)）：\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} \\cdot \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{z}_2} \\cdot W_2^T \\cdot \\sigma'(\\mathbf{z}_1) \\cdot \\mathbf{x}^T \\quad (\\mathbf{z}_1 = W_1 \\mathbf{x} + b_1)\n",
    "\\]\n",
    "   - 多了“输出层到隐藏层的权重传递”和“隐藏层激活函数导数”，链式法则的“链条”更长。\n",
    "\n",
    "#### （2）复杂激活函数的梯度计算\n",
    "AI 中常用的激活函数（如 ReLU 变体、GELU、Swish）均为复合函数，其梯度计算依赖链式法则：\n",
    "- **GELU 函数**（Transformer 常用）：\\( \\text{GELU}(x) = \\frac{1}{2}x \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right] \\)\n",
    "  - 分解：\\( u = \\frac{x}{\\sqrt{2}}, v = \\text{erf}(u), w = 1 + v, y = \\frac{1}{2}x \\cdot w \\)\n",
    "  - 梯度（结合乘积法则+链式法则）：\n",
    "\\[\n",
    "\\text{GELU}'(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right] + \\frac{x}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n",
    "\\]\n",
    "- **Leaky ReLU 函数**：\\( \\text{LeakyReLU}(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\leq 0 \\end{cases} \\)\n",
    "  - 分解：\\( u = x, y = \\max(\\alpha u, u) \\)\n",
    "  - 梯度：\\( \\text{LeakyReLU}'(x) = \\begin{cases} 1 & x > 0 \\\\ \\alpha & x \\leq 0 \\end{cases} \\)（避免 ReLU 负区间梯度消失）。\n",
    "\n",
    "#### （3）优化算法中的梯度链式传递\n",
    "梯度下降、Adam 等优化算法的核心是“参数梯度”，而参数通常嵌套在多层复合函数中：\n",
    "- 示例：线性回归模型 \\( \\hat{y} = Wx + b \\)，MSE 损失 \\( L = \\frac{1}{2}(\\hat{y} - y)^2 \\)，对权重 \\( W \\) 的梯度：\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W} = (\\hat{y} - y) \\cdot x\n",
    "\\]\n",
    "  - 其中 \\( \\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y \\)（损失对预测值的梯度），\\( \\frac{\\partial \\hat{y}}{\\partial W} = x \\)（预测值对权重的梯度），链式法则将两者连接。\n",
    "\n",
    "#### （4）深度学习中的自动微分（AutoGrad）\n",
    "PyTorch、TensorFlow 等框架的自动微分功能，本质是**对链式法则的自动化实现**：\n",
    "- 前向传播时，框架记录函数的复合结构（如 \\( y = f(g(h(x))) \\) 的调用顺序）；\n",
    "- 反向传播时，按“从输出到输入”的顺序，自动应用链式法则计算各参数的梯度，无需手动推导。\n",
    "\n",
    "\n",
    "## 五、拓展联系\n",
    "### 1. 反函数求导与复合函数求导的结合\n",
    "AI 中常遇到“反函数+复合函数”的组合，需同时应用两种法则：\n",
    "- 示例：求 \\( y = \\ln(\\sin x^2) \\) 的导数（对数函数是指数函数的反函数，整体为复合函数）：\n",
    "  1. 分解复合结构：\\( y = \\ln u, u = \\sin v, v = x^2 \\)；\n",
    "  2. 应用链式法则：\\( \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dv} \\cdot \\frac{dv}{dx} \\)；\n",
    "  3. 各层导数：\\( \\frac{dy}{du} = \\frac{1}{u} \\)（反函数求导：\\( \\ln u \\) 是 \\( e^u \\) 的反函数），\\( \\frac{du}{dv} = \\cos v \\)，\\( \\frac{dv}{dx} = 2x \\)；\n",
    "  4. 代入合并：\\( \\frac{dy}{dx} = \\frac{1}{\\sin x^2} \\cdot \\cos x^2 \\cdot 2x = 2x \\cot x^2 \\)。\n",
    "\n",
    "### 2. 与高阶导数的关联（AI 高阶优化）\n",
    "链式法则可推广到高阶导数，AI 中的二阶优化算法（如牛顿法、L-BFGS）需用到二阶导数（Hessian 矩阵）：\n",
    "- 示例：求 \\( y = \\sigma(Wx + b) \\) 的二阶导数（对 \\( x \\)）：\n",
    "  1. 一阶导数：\\( y' = \\sigma'(Wx + b) \\cdot W = \\sigma(Wx + b)(1 - \\sigma(Wx + b)) \\cdot W \\)；\n",
    "  2. 二阶导数（结合乘积法则+链式法则）：\n",
    "\\[\n",
    "y'' = \\left[ \\sigma'(Wx + b)(1 - \\sigma(Wx + b)) - \\sigma(Wx + b)\\sigma'(Wx + b) \\right] \\cdot W^2 = \\sigma(Wx + b)(1 - \\sigma(Wx + b))(1 - 2\\sigma(Wx + b)) \\cdot W^2\n",
    "\\]\n",
    "- 应用场景：Hessian 矩阵的元素是二阶导数，可用于加速优化收敛，但计算成本较高。\n",
    "\n",
    "### 3. Python 代码验证（实战演示）\n",
    "使用 `sympy` 验证求导法则，并模拟神经网络激活函数梯度与反向传播的核心步骤：\n",
    "\n",
    "```python\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. 符号计算验证反函数和复合函数求导\n",
    "x = sp.Symbol('x')\n",
    "W, b = sp.symbols('W b', constants=True)\n",
    "\n",
    "# （1）反函数求导验证：y = e^x 的反函数 x = ln y\n",
    "y_exp = sp.exp(x)\n",
    "inv_y = sp.ln(x)  # 反函数（变量统一为 x）\n",
    "deriv_exp = sp.diff(y_exp, x)\n",
    "deriv_inv_exp = sp.diff(inv_y, x)\n",
    "print(\"反函数求导验证（e^x 与 ln x）：\")\n",
    "print(f\"e^x 的导数：{deriv_exp}\")\n",
    "print(f\"ln x 的导数：{deriv_inv_exp}\")\n",
    "print(f\"是否满足倒数关系（x>0）：{deriv_inv_exp == 1 / deriv_exp.subs(x, sp.ln(x))}\\n\")\n",
    "\n",
    "# （2）复合函数求导验证：y = sigma(Wx + b)（Sigmoid 激活）\n",
    "def sigmoid_sympy(u):\n",
    "    return 1 / (1 + sp.exp(-u))\n",
    "\n",
    "u = W * x + b\n",
    "y_sigmoid = sigmoid_sympy(u)\n",
    "deriv_sigmoid_manual = sigmoid_sympy(u) * (1 - sigmoid_sympy(u)) * W  # 链式法则\n",
    "deriv_sigmoid_auto = sp.diff(y_sigmoid, x)\n",
    "print(\"复合函数求导验证（Sigmoid 激活）：\")\n",
    "print(f\"手动计算（链式法则）：{deriv_sigmoid_manual.simplify()}\")\n",
    "print(f\"自动计算：{deriv_sigmoid_auto.simplify()}\")\n",
    "print(f\"是否相等：{deriv_sigmoid_manual.simplify() == deriv_sigmoid_auto.simplify()}\\n\")\n",
    "\n",
    "# 2. 实战：用 PyTorch 自动微分模拟反向传播（链式法则应用）\n",
    "# 定义简单神经网络（1 隐藏层 + Sigmoid 激活）\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, 10)  # 输入层→隐藏层\n",
    "        self.fc2 = torch.nn.Linear(10, 1)   # 隐藏层→输出层\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.sigmoid(self.fc1(x))  # 隐藏层：复合函数（Sigmoid(W1x + b1)）\n",
    "        y_hat = self.sigmoid(self.fc2(h))  # 输出层：复合函数（Sigmoid(W2h + b2)）\n",
    "        return y_hat, h\n",
    "\n",
    "# 模拟数据\n",
    "x = torch.tensor([[1.0], [2.0], [3.0]], requires_grad=False)\n",
    "y_true = torch.tensor([[0.2], [0.5], [0.8]], requires_grad=False)\n",
    "\n",
    "# 初始化模型和损失函数\n",
    "model = SimpleNN()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# 前向传播\n",
    "y_hat, h = model(x)\n",
    "loss = mse_loss(y_hat, y_true)\n",
    "\n",
    "# 反向传播（自动应用链式法则计算梯度）\n",
    "loss.backward()\n",
    "\n",
    "# 输出梯度结果（验证链式法则的梯度传递）\n",
    "print(\"PyTorch 自动微分（反向传播）结果：\")\n",
    "print(f\"损失值：{loss.item():.6f}\")\n",
    "print(f\"输出层权重 fc2.weight 的梯度形状：{model.fc2.weight.grad.shape}\")\n",
    "print(f\"隐藏层权重 fc1.weight 的梯度形状：{model.fc1.weight.grad.shape}\")\n",
    "print(f\"隐藏层输出 h 的梯度（链式传递中间结果）：{h.grad[0][:5]}...\\n\")  # 打印前 5 个元素\n",
    "\n",
    "# 3. 实战：手动计算 GELU 函数梯度并验证\n",
    "def gelu(x):\n",
    "    \"\"\"GELU 激活函数：(1/2)x[1 + erf(x/√2)]\"\"\"\n",
    "    return 0.5 * x * (1 + torch.erf(x / np.sqrt(2)))\n",
    "\n",
    "def gelu_deriv_manual(x):\n",
    "    \"\"\"手动计算 GELU 梯度（链式法则+乘积法则）\"\"\"\n",
    "    sqrt_2_over_pi = np.sqrt(2 / np.pi)\n",
    "    exp_term = torch.exp(-0.5 * x**2)\n",
    "    return 0.5 * (1 + torch.erf(x / np.sqrt(2))) + 0.5 * x * sqrt_2_over_pi * exp_term\n",
    "\n",
    "# 数值验证\n",
    "x_val = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "gelu_val = gelu(x_val)\n",
    "gelu_val.sum().backward()  # 自动求导\n",
    "gelu_deriv_auto = x_val.grad\n",
    "gelu_deriv_manual_val = gelu_deriv_manual(x_val)\n",
    "\n",
    "print(\"GELU 函数梯度验证（手动 vs 自动）：\")\n",
    "for x, manual, auto in zip(x_val.detach().numpy(), gelu_deriv_manual_val.detach().numpy(), gelu_deriv_auto.detach().numpy()):\n",
    "    print(f\"x={x:.1f}: 手动计算={manual:.6f}, 自动计算={auto:.6f}, 误差={abs(manual-auto):.6e}\")\n",
    "```\n",
    "\n",
    "### 4. 学习建议（衔接后续内容）\n",
    "1. **链式法则优先级**：AI 中函数几乎都是复合函数，需将链式法则视为“求导第一法则”，遇到复合结构先分解中间变量，再结合四则运算求导；\n",
    "2. **反向传播视角**：学习链式法则时，始终以“反向传播”为目标——例如，思考“输出层损失如何通过链式法则传递到输入层参数”，建立“梯度传递链条”的直觉；\n",
    "3. **工具辅助验证**：用 `sympy` 验证手动推导的梯度公式，用 PyTorch/TensorFlow 的自动微分功能验证复杂函数（如 GELU、Swish）的梯度，避免推导错误；\n",
    "4. **预习衔接**：下一节将学习“高阶导数与微分”，需重点关注“复合函数的二阶导数计算”（如激活函数的二阶导数对优化的影响），以及“微分在数值计算中的应用”（如梯度近似）。"
   ],
   "id": "93b34d5c53c95a57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
