{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9987b425d99ba047"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 初等函数导数（CS/AI 专项笔记·精研版）\n",
    "## 前言\n",
    "初等函数是由基本初等函数通过有限次四则运算、复合运算构成的函数，其导数是微积分在AI领域应用的**核心工具**。从深度学习中激活函数的梯度计算，到优化算法的参数更新，再到损失函数的误差反向传播，无一不依赖初等函数导数的计算规则。本章将系统梳理初等函数导数的计算逻辑，涵盖四则运算求导、复合函数求导（链式法则），结合AI高频场景拆解典型案例，配套代码验证与易错点辨析，形成适配Jupyter归档的结构化学习笔记。\n",
    "\n",
    "## 1. 初等函数的范畴界定与导数计算原则\n",
    "### 1.1 严格范畴\n",
    "初等函数 = 基本初等函数（常数、幂、指数、对数、三角、反三角函数）+ 有限次四则运算（加、减、乘、除）+ 有限次复合运算。\n",
    "- 示例：Sigmoid函数 $\\sigma(x)=\\frac{1}{1+e^{-x}}$（指数函数+四则运算+复合运算）、$\\ln(\\sin x^2)$（对数+三角+幂函数复合）。\n",
    "\n",
    "### 1.2 导数计算核心原则\n",
    "1.  **基础先行**：先掌握基本初等函数的导函数公式（前文已详解）；\n",
    "2.  **法则复用**：利用四则运算求导法则、复合函数求导法则（链式法则），将复杂初等函数拆解为简单函数求导；\n",
    "3.  **AI适配**：优先关注复合函数导数（链式法则是反向传播的核心），简化纯理论推导，强化工程落地逻辑。\n",
    "\n",
    "## 2. 初等函数导数的核心计算法则\n",
    "### 2.1 四则运算求导法则（基础组合工具）\n",
    "设 $u(x)$、$v(x)$ 均为可导的初等函数，$k$ 为常数，四则运算的导数遵循以下固定规则，是构建复杂初等函数导数的基础。\n",
    "```html\n",
    "<table style=\"width:100%; border-collapse: collapse; margin: 16px 0; font-size: 14px;\">\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #f5f5f5;\">\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">运算类型</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">导数公式</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">CS/AI 应用场景</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">常数倍运算</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(k \\cdot u)' = k \\cdot u'$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">损失函数正则项求导（如 $L_2$ 正则 $\\lambda \\cdot \\theta^2$）</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">加减运算</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(u \\pm v)' = u' \\pm v'$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">多任务学习联合损失函数求导（如 $L = L_{\\text{cls}} + L_{\\text{reg}}$）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">乘积运算</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(u \\cdot v)' = u'v + uv'$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">线性回归模型中 $wx + b$ 的权重梯度计算</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">商运算</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$\\left( \\frac{u}{v} \\right)' = \\frac{u'v - uv'}{v^2} (v \\neq 0)$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">Sigmoid函数、Tanh函数的导数推导</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "#### 证明（乘积法则，AI推导核心）\n",
    "1.  由导数定义：$(u \\cdot v)' = \\lim_{\\Delta x \\to 0} \\frac{u(x+\\Delta x)v(x+\\Delta x) - u(x)v(x)}{\\Delta x}$；\n",
    "2.  拆分分子：$u(x+\\Delta x)v(x+\\Delta x) - u(x)v(x) = u(x+\\Delta x)[v(x+\\Delta x)-v(x)] + v(x)[u(x+\\Delta x)-u(x)]$；\n",
    "3.  利用极限四则运算与函数连续性：$\\lim_{\\Delta x \\to 0} u(x+\\Delta x) = u(x)$，最终得 $u'v + uv'$。\n",
    "\n",
    "### 2.2 复合函数求导法则（链式法则，AI核心）\n",
    "复合函数是初等函数在AI中最主要的存在形式（如激活函数、损失函数），链式法则是其导数计算的灵魂，更是**反向传播算法的数学基石**。\n",
    "#### 2.2.1 严格表述\n",
    "设 $y = f(u)$（外层函数），$u = g(x)$（内层函数），且 $f(u)$ 在 $u = g(x)$ 处可导，$g(x)$ 在 $x$ 处可导，则复合函数 $y = f(g(x))$ 的导数为：\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} \\quad \\text{或} \\quad [f(g(x))]' = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "#### 2.2.2 多重复合拓展（深度学习高频）\n",
    "若存在多层复合 $y = f(h(g(x)))$，则链式法则可逐层应用：\n",
    "$$\\frac{dy}{dx} = f'(h(g(x))) \\cdot h'(g(x)) \\cdot g'(x)$$\n",
    "这正是深层神经网络中，梯度从输出层反向传播至输入层的核心计算逻辑。\n",
    "\n",
    "### 2.3 隐函数求导法则（复杂初等函数补充）\n",
    "对于无法直接显化 $y = f(x)$ 的隐式初等函数 $F(x,y)=0$，可通过**两边对 $x$ 求导**，将 $y$ 视为 $x$ 的复合函数，结合链式法则求解 $\\frac{dy}{dx}$。\n",
    "- 示例：求 $x^2 + y^2 = 1$ 的导数，两边求导得 $2x + 2y \\cdot y' = 0$，解得 $y' = -\\frac{x}{y}$；\n",
    "- AI应用：复杂激活函数的隐式表达式求导（如自定义非线性激活函数）。\n",
    "\n",
    "## 3. AI高频初等函数导数推导案例\n",
    "选取深度学习中最常用的初等函数（激活函数、损失函数），完整拆解导数推导过程，强化理论与工程的衔接。\n",
    "\n",
    "### 3.1 案例1：Sigmoid函数（复合+商运算）\n",
    "#### 函数表达式\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "#### 导数推导\n",
    "1.  拆解为复合+商运算：令 $u = 1 + e^{-x}$，则 $\\sigma(x) = u^{-1}$；\n",
    "2.  外层函数求导：$\\frac{d\\sigma}{du} = -u^{-2}$；\n",
    "3.  内层函数求导：$\\frac{du}{dx} = -e^{-x}$；\n",
    "4.  链式法则结合：$\\sigma'(x) = -u^{-2} \\cdot (-e^{-x}) = \\frac{e^{-x}}{(1 + e^{-x})^2}$；\n",
    "5.  化简（AI工程常用）：$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$。\n",
    "#### AI价值\n",
    "导数取值范围 $(0, \\frac{1}{4}]$，解释了深层网络中Sigmoid函数梯度消失的原因，指导激活函数选型优化。\n",
    "\n",
    "### 3.2 案例2：Tanh函数（复合+商运算）\n",
    "#### 函数表达式\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "#### 导数推导\n",
    "1.  商运算法则：令 $u = e^x - e^{-x}$，$v = e^x + e^{-x}$，则 $\\tanh'(x) = \\frac{u'v - uv'}{v^2}$；\n",
    "2.  分别求导：$u' = e^x + e^{-x}$，$v' = e^x - e^{-x}$；\n",
    "3.  代入化简：$\\tanh'(x) = \\frac{(e^x + e^{-x})^2 - (e^x - e^{-x})^2}{(e^x + e^{-x})^2} = 1 - \\tanh^2(x)$。\n",
    "#### AI价值\n",
    "梯度取值范围 $(0,1]$，稳定性优于Sigmoid，成为深层网络的常用激活函数。\n",
    "\n",
    "### 3.3 案例3：交叉熵损失函数（对数+复合运算）\n",
    "#### 函数表达式（二分类场景）\n",
    "$$L = -\\sum_{i=1}^n [y_i \\ln \\sigma(x_i) + (1 - y_i) \\ln(1 - \\sigma(x_i))]$$\n",
    "#### 导数推导（对单个 $x_i$）\n",
    "1.  代入Sigmoid函数：$L = -[y \\ln \\sigma + (1 - y) \\ln(1 - \\sigma)]$；\n",
    "2.  链式法则求导：$\\frac{\\partial L}{\\partial x} = -\\left[ y \\cdot \\frac{1}{\\sigma} \\cdot \\sigma' - (1 - y) \\cdot \\frac{1}{1 - \\sigma} \\cdot \\sigma' \\right]$；\n",
    "3.  代入 $\\sigma'(x) = \\sigma(1 - \\sigma)$：化简得 $\\frac{\\partial L}{\\partial x} = \\sigma(x) - y$。\n",
    "#### AI价值\n",
    "导数形式极简，避免了梯度消失，是二分类模型的首选损失函数。\n",
    "\n",
    "### 3.4 案例4：幂函数复合（学习率衰减函数）\n",
    "#### 函数表达式（多项式衰减）\n",
    "$$\\eta(t) = \\eta_0 (1 + kt)^{-\\frac{1}{2}}$$\n",
    "#### 导数推导\n",
    "1.  复合函数拆解：令 $u = 1 + kt$，$y = \\eta_0 u^{-\\frac{1}{2}}$；\n",
    "2.  链式法则：$\\frac{d\\eta}{dt} = \\eta_0 \\cdot \\left( -\\frac{1}{2} u^{-\\frac{3}{2}} \\right) \\cdot k = -\\frac{k \\eta_0}{2(1 + kt)^{\\frac{3}{2}}}$。\n",
    "#### AI价值\n",
    "导数为负，保证学习率随迭代次数 $t$ 单调递减，且衰减速度逐渐放缓，平衡收敛速度与精度。\n",
    "\n",
    "## 4. 工程实现（Python 导数计算工具箱）\n",
    "实现AI高频初等函数的导数计算，并通过数值差分法验证解析导数的正确性，可直接嵌入深度学习模型的反向传播模块。\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-6):\n",
    "    \"\"\"有限差分法计算数值导数（验证用）\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# 定义AI高频初等函数及其解析导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    sigma = sigmoid(x)\n",
    "    return sigma * (1 - sigma)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    tanh_x = np.tanh(x)\n",
    "    return 1 - tanh_x ** 2\n",
    "\n",
    "def cross_entropy_deriv(x, y):\n",
    "    \"\"\"交叉熵损失对x的导数（二分类）\"\"\"\n",
    "    sigma = sigmoid(x)\n",
    "    return sigma - y\n",
    "\n",
    "def lr_decay_deriv(t, eta0, k):\n",
    "    \"\"\"学习率衰减函数对t的导数\"\"\"\n",
    "    return - (k * eta0) / (2 * (1 + k * t) ** (3/2))\n",
    "\n",
    "# 验证核心函数导数\n",
    "x = np.array([-1, 0, 1])\n",
    "y = np.array([0, 1, 0])\n",
    "t = np.array([0, 10, 100])\n",
    "eta0, k = 0.1, 0.01\n",
    "\n",
    "# 验证Sigmoid导数\n",
    "sigmoid_analytic = sigmoid_deriv(x)\n",
    "sigmoid_numerical = numerical_derivative(sigmoid, x)\n",
    "print(\"Sigmoid函数导数验证：\")\n",
    "print(f\"  解析导数: {sigmoid_analytic.round(6)}\")\n",
    "print(f\"  数值导数: {sigmoid_numerical.round(6)}\\n\")\n",
    "\n",
    "# 验证交叉熵损失导数\n",
    "ce_analytic = cross_entropy_deriv(x, y)\n",
    "print(\"交叉熵损失导数验证：\")\n",
    "print(f\"  解析导数: {ce_analytic.round(6)}\\n\")\n",
    "\n",
    "# 验证学习率衰减导数\n",
    "lr_analytic = lr_decay_deriv(t, eta0, k)\n",
    "print(\"学习率衰减函数导数验证：\")\n",
    "print(f\"  解析导数: {lr_analytic.round(8)}\")\n",
    "```\n",
    "\n",
    "## 5. 常见误区与易错点辨析\n",
    "```html\n",
    "<table style=\"width:100%; border-collapse: collapse; margin: 16px 0; font-size: 14px;\">\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #f5f5f5;\">\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">易错点</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">错误认知</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">正确结论</th>\n",
    "      <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd; font-weight: 600;\">AI 避坑措施</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">复合函数漏层求导</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(f(g(x)))' = f'(g(x))$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">必须乘内层导数：$f'(g(x)) \\cdot g'(x)$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">反向传播时，逐层记录每一层导数，画计算图辅助拆解</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">商运算分子符号错误</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$\\left( \\frac{u}{v} \\right)' = \\frac{uv' - u'v}{v^2}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">分子为 $u'v - uv'$，符号不可颠倒</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">推导Sigmoid/Tanh导数后，用x=0处的特殊值验证（如$\\sigma'(0)=0.25$）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">指数复合函数导数漏符号</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(e^{-x})' = e^{-x}$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">$(e^{-x})' = -e^{-x}$，内层导数为-1</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">计算时先写外层导数，再逐层乘内层导数，避免跳步</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fafafa;\">\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">混淆显函数与隐函数求导</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">隐函数求导无需考虑y的导数</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">隐函数中y是x的复合函数，需用链式法则求$y'$</td>\n",
    "      <td style=\"padding: 12px; border: 1px solid #ddd;\">自定义复杂激活函数时，优先显化表达式，避免隐函数求导出错</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "## 6. 拓展联系与AI核心应用场景\n",
    "### 6.1 与深度学习的核心关联\n",
    "1.  **反向传播算法**：链式法则是反向传播的数学基础，通过逐层计算复合函数的导数，将损失函数的梯度从输出层传递至输入层，实现所有参数的更新；\n",
    "2.  **激活函数设计**：初等函数的导数特性决定激活函数的性能，如ReLU的分段导数、Sigmoid的梯度饱和特性，直接影响模型的训练效果；\n",
    "3.  **优化算法效率**：简洁的导数表达式（如交叉熵损失的导数 $\\sigma(x)-y$）可大幅降低计算复杂度，提升优化算法的迭代速度。\n",
    "\n",
    "### 6.2 与后续知识的衔接\n",
    "1.  **高阶导数**：初等函数的二阶导数是牛顿法、Hessian矩阵的基础，用于加速优化收敛；\n",
    "2.  **偏导数**：多变量初等函数的导数（如 $f(x_1,x_2)=\\ln(x_1^2 + x_2^2)$）是梯度向量的核心，拓展至深度学习的多维参数空间；\n",
    "3.  **微分方程**：在时序预测、强化学习中，常通过初等函数导数构建微分方程，描述系统的动态变化。\n",
    "\n",
    "## 7. 学习建议（CS/AI 方向专属）\n",
    "1.  **锚定链式法则核心**：链式法则是初等函数导数在AI中应用的重中之重，需通过Sigmoid、Tanh等函数的反复推导，熟练掌握多层复合的求导逻辑；\n",
    "2.  **强化工程验证**：每推导一个函数的导数，立即用Python数值差分法验证，形成“推导-验证-纠错”的闭环，避免理论与工程脱节；\n",
    "3.  **结合模型场景记忆**：将导数公式与具体AI场景绑定，如看到“商运算”就联想到Sigmoid，看到“链式法则”就联想到反向传播，避免孤立记忆；\n",
    "4.  **提前铺垫偏导数**：初等函数的单变量导数是多变量偏导数的基础，学习时主动思考“多参数场景下如何拓展”，为后续深度学习的梯度计算打基础。\n",
    "\n",
    "是否需要我针对**多变量初等函数的偏导数**或**反向传播算法中初等函数导数的完整应用流程**，提供更详细的案例推导和代码实现？"
   ],
   "id": "a6524b5c45f002f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
