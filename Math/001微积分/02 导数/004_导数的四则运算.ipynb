{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f5b1792ac14cbe5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 微积分核心笔记（AI/CS 方向）：导数的四则运算\n",
    "## 一、精确定义\n",
    "设函数 \\( u(x) \\) 和 \\( v(x) \\) 在点 \\( x \\) 处均可导（记 \\( u' = u'(x) \\)，\\( v' = v'(x) \\)），则它们的和、差、积、商（分母不为零）在点 \\( x \\) 处也可导，且满足以下四则运算法则：\n",
    "\n",
    "| 运算类型 | 导数公式 | 备注 |\n",
    "|----------|----------|------|\n",
    "| **和差法则** | \\( (u \\pm v)' = u' \\pm v' \\) | 可推广到有限个函数的和差 |\n",
    "| **乘积法则** | \\( (u \\cdot v)' = u'v + uv' \\) | 特别地，若 \\( v = C \\)（常数），则 \\( (Cu)' = Cu' \\)（常数因子法则） |\n",
    "| **商法则** | \\( \\left( \\frac{u}{v} \\right)' = \\frac{u'v - uv'}{v^2} \\) | 要求 \\( v(x) \\neq 0 \\)；特别地，\\( \\left( \\frac{1}{v} \\right)' = -\\frac{v'}{v^2} \\) |\n",
    "\n",
    "**推广形式**：\n",
    "- 有限个函数乘积的导数：\\( (u_1 u_2 \\cdots u_n)' = u_1' u_2 \\cdots u_n + u_1 u_2' \\cdots u_n + \\cdots + u_1 u_2 \\cdots u_n' \\)\n",
    "- 常数与函数的乘积：\\( (C \\cdot u)' = C \\cdot u' \\)（\\( C \\) 为常数，本质是乘积法则的特例）\n",
    "\n",
    "\n",
    "## 二、公式推导\n",
    "### 1. 和差法则推导（基于导数定义）\n",
    "要证明 \\( (u + v)' = u' + v' \\)，按导数定义展开：\n",
    "\\[\n",
    "\\begin{align*}\n",
    "(u + v)'(x) &= \\lim_{\\Delta x \\to 0} \\frac{[u(x+\\Delta x) + v(x+\\Delta x)] - [u(x) + v(x)]}{\\Delta x} \\\\\n",
    "&= \\lim_{\\Delta x \\to 0} \\left( \\frac{u(x+\\Delta x) - u(x)}{\\Delta x} + \\frac{v(x+\\Delta x) - v(x)}{\\Delta x} \\right) \\\\\n",
    "&= \\lim_{\\Delta x \\to 0} \\frac{u(x+\\Delta x) - u(x)}{\\Delta x} + \\lim_{\\Delta x \\to 0} \\frac{v(x+\\Delta x) - v(x)}{\\Delta x} \\\\\n",
    "&= u'(x) + v'(x)\n",
    "\\end{align*}\n",
    "\\]\n",
    "同理可证 \\( (u - v)' = u' - v' \\)（将上式中 \\( v \\) 替换为 \\( -v \\) 即可）。\n",
    "\n",
    "### 2. 乘积法则推导\n",
    "要证明 \\( (u \\cdot v)' = u'v + uv' \\)，引入增量展开：\n",
    "\\[\n",
    "\\begin{align*}\n",
    "(u \\cdot v)'(x) &= \\lim_{\\Delta x \\to 0} \\frac{u(x+\\Delta x)v(x+\\Delta x) - u(x)v(x)}{\\Delta x} \\\\\n",
    "&= \\lim_{\\Delta x \\to 0} \\frac{u(x+\\Delta x)v(x+\\Delta x) - u(x)v(x+\\Delta x) + u(x)v(x+\\Delta x) - u(x)v(x)}{\\Delta x} \\\\\n",
    "&= \\lim_{\\Delta x \\to 0} \\left( v(x+\\Delta x) \\cdot \\frac{u(x+\\Delta x) - u(x)}{\\Delta x} + u(x) \\cdot \\frac{v(x+\\Delta x) - v(x)}{\\Delta x} \\right)\n",
    "\\end{align*}\n",
    "\\]\n",
    "由于 \\( v(x) \\) 可导故连续，得 \\( \\lim_{\\Delta x \\to 0} v(x+\\Delta x) = v(x) \\)，因此：\n",
    "\\[\n",
    "(u \\cdot v)'(x) = v(x) \\cdot u'(x) + u(x) \\cdot v'(x) = u'v + uv'\n",
    "\\]\n",
    "\n",
    "### 3. 商法则推导\n",
    "要证明 \\( \\left( \\frac{u}{v} \\right)' = \\frac{u'v - uv'}{v^2} \\)（\\( v(x) \\neq 0 \\)），结合乘积法则和倒数的导数：\n",
    "1. 先证倒数的导数：设 \\( y = \\frac{1}{v(x)} \\)，则：\n",
    "\\[\n",
    "y' = \\lim_{\\Delta x \\to 0} \\frac{\\frac{1}{v(x+\\Delta x)} - \\frac{1}{v(x)}}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac{v(x) - v(x+\\Delta x)}{\\Delta x \\cdot v(x)v(x+\\Delta x)} = -\\frac{v'(x)}{v^2(x)}\n",
    "\\]\n",
    "2. 再将 \\( \\frac{u}{v} \\) 视为 \\( u \\cdot \\frac{1}{v} \\)，应用乘积法则：\n",
    "\\[\n",
    "\\left( \\frac{u}{v} \\right)' = u' \\cdot \\frac{1}{v} + u \\cdot \\left( \\frac{1}{v} \\right)' = \\frac{u'}{v} - \\frac{uv'}{v^2} = \\frac{u'v - uv'}{v^2}\n",
    "\\]\n",
    "\n",
    "\n",
    "## 三、原理分析\n",
    "### 1. 核心本质\n",
    "导数的四则运算本质是「变化率的组合规则」：\n",
    "- 和差法则：函数和/差的变化率 = 变化率的和/差（线性运算的变化率传递）；\n",
    "- 乘积法则：函数乘积的变化率 = 第一个函数的变化率乘第二个函数 + 第一个函数乘第二个函数的变化率（体现“相互影响”的非线性效应）；\n",
    "- 商法则：函数商的变化率 = （分子变化率乘分母 - 分子乘分母变化率）除以分母的平方（抵消分母变化带来的影响）。\n",
    "\n",
    "### 2. 关键注意事项\n",
    "1. **乘积法则不可遗漏项**：切勿误记为 \\( (uv)' = u'v' \\)，需保留两项（\\( u'v + uv' \\)），这是 AI 中激活函数组合（如 \\( f(x) = x \\cdot \\sigma(x) \\)）求导的关键；\n",
    "2. **商法则的符号与分母**：分子是“分子导乘分母 - 分子乘分母导”（注意减号顺序），分母是“分母的平方”，AI 中分式激活函数（如 Swish 的变体）需严格遵循；\n",
    "3. **有限推广与无限限制**：法则仅适用于有限个函数的和差积，无限个函数的情况需结合级数收敛性分析（AI 中神经网络的层叠是有限的，故可直接应用）。\n",
    "\n",
    "### 3. 基础例题（含 AI 常用函数）\n",
    "| 函数形式 | 运算类型 | 导数计算（应用四则法则） | 应用场景 |\n",
    "|----------|----------|--------------------------|----------|\n",
    "| \\( f(x) = 3x^2 + 2e^x - \\ln x \\) | 和差+常数因子 | \\( f'(x) = 3 \\cdot 2x + 2e^x - \\frac{1}{x} = 6x + 2e^x - \\frac{1}{x} \\) | 多项式+指数+对数组合的损失函数 |\n",
    "| \\( f(x) = x \\cdot \\sin x \\) | 乘积 | \\( f'(x) = 1 \\cdot \\sin x + x \\cdot \\cos x = \\sin x + x \\cos x \\) | 周期性信号的权重调制 |\n",
    "| \\( f(x) = \\frac{x^2}{1 + e^x} \\) | 商 | \\( f'(x) = \\frac{2x \\cdot (1 + e^x) - x^2 \\cdot e^x}{(1 + e^x)^2} = \\frac{2x + 2x e^x - x^2 e^x}{(1 + e^x)^2} \\) | 分式激活函数 |\n",
    "| \\( f(x) = 5x^3 \\cdot \\sigma(x) \\)（\\( \\sigma \\) 为 Sigmoid） | 乘积+常数因子 | \\( f'(x) = 15x^2 \\cdot \\sigma(x) + 5x^3 \\cdot \\sigma'(x) \\) | 带权重的 Sigmoid 激活 |\n",
    "\n",
    "\n",
    "## 四、CS/AI 应用场景\n",
    "导数的四则运算是 AI 模型训练的「基础工具」，所有涉及“组合函数求导”的场景（如损失函数、激活函数、模型参数梯度计算）均需用到，核心场景如下：\n",
    "\n",
    "### 1. 复合激活函数的导数计算\n",
    "AI 中常用的激活函数多为“基本初等函数的组合”，需结合四则运算+链式法则求导：\n",
    "- **Swish 函数**（深度学习常用激活）：\\( \\text{Swish}(x) = x \\cdot \\sigma(x) \\)（\\( \\sigma \\) 为 Sigmoid），其导数为：\n",
    "  \\[\n",
    "  \\text{Swish}'(x) = \\sigma(x) + x \\cdot \\sigma'(x) = \\sigma(x) + x \\cdot \\sigma(x)(1 - \\sigma(x)) = \\sigma(x)(1 + x - x \\sigma(x))\n",
    "  \\]\n",
    "  （应用乘积法则：\\( u = x \\)，\\( v = \\sigma(x) \\)，故 \\( (uv)' = u'v + uv' \\)）；\n",
    "- **ELU 函数**（抗梯度消失）：\\( \\text{ELU}(x) = \\begin{cases} x & x > 0 \\\\ \\alpha(e^x - 1) & x \\leq 0 \\end{cases} \\)，导数为：\n",
    "  \\[\n",
    "  \\text{ELU}'(x) = \\begin{cases} 1 & x > 0 \\\\ \\alpha e^x & x \\leq 0 \\end{cases}\n",
    "  \\]\n",
    "  （应用和差+常数因子法则：\\( \\alpha(e^x - 1) = \\alpha e^x - \\alpha \\)，导数为 \\( \\alpha e^x - 0 = \\alpha e^x \\)）。\n",
    "\n",
    "### 2. 损失函数的梯度计算\n",
    "模型训练的核心是最小化损失函数，损失函数通常是“预测值与真实值的组合”，其梯度计算依赖四则运算：\n",
    "- **均方误差（MSE）损失**：\\( L = \\frac{1}{2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\)（\\( y_i \\) 真实值，\\( \\hat{y}_i = f(x_i; \\theta) \\) 预测值），对参数 \\( \\theta \\) 的梯度：\n",
    "  \\[\n",
    "  \\frac{\\partial L}{\\partial \\theta} = \\sum_{i=1}^n (y_i - \\hat{y}_i) \\cdot \\left( -\\frac{\\partial \\hat{y}_i}{\\partial \\theta} \\right)\n",
    "  \\]\n",
    "  （应用和差法则：求和的导数=导数的和；常数因子法则：\\( \\frac{1}{2} \\) 提到导数外）；\n",
    "- **交叉熵损失（二分类）**：\\( L = -[y \\ln \\hat{y} + (1 - y) \\ln(1 - \\hat{y})] \\)，对预测值 \\( \\hat{y} \\) 的导数：\n",
    "  \\[\n",
    "  \\frac{\\partial L}{\\partial \\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)\n",
    "  \\]\n",
    "  （应用和差法则+商法则：\\( (\\ln \\hat{y})' = \\frac{1}{\\hat{y}} \\)，\\( (\\ln(1 - \\hat{y}))' = -\\frac{1}{1 - \\hat{y}} \\)）。\n",
    "\n",
    "### 3. 神经网络层间梯度传递\n",
    "神经网络的每一层输出都是“权重×输入+偏置”的组合，其梯度计算需结合四则运算与链式法则：\n",
    "- 简化示例：单层全连接层 \\( y = Wx + b \\)（\\( W \\) 权重，\\( b \\) 偏置，\\( x \\) 输入），损失 \\( L(y, \\hat{y}) \\) 对 \\( W \\) 和 \\( b \\) 的梯度：\n",
    "  \\[\n",
    "  \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot x^T, \\quad \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y}\n",
    "  \\]\n",
    "  （应用和差法则：\\( Wx + b \\) 的导数= \\( Wx \\) 的导数+ \\( b \\) 的导数；乘积法则：\\( Wx \\) 对 \\( W \\) 的偏导为 \\( x^T \\)）。\n",
    "\n",
    "### 4. 优化算法中的参数更新\n",
    "梯度下降类算法（SGD、Adam 等）的核心是“用梯度更新参数”，而梯度计算的正确性依赖四则运算：\n",
    "- SGD 参数更新公式：\\( \\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta L(\\theta_t) \\)，其中 \\( \\nabla_\\theta L(\\theta_t) \\) 是损失函数对参数的梯度，其计算过程需反复应用四则运算。\n",
    "\n",
    "\n",
    "## 五、拓展联系\n",
    "### 1. 与链式法则的衔接（后续核心应用）\n",
    "AI 中绝大多数函数是“四则运算+复合函数”的组合（如 \\( f(x) = \\sin(x^2 + e^x) \\)、\\( f(x) = (x^3 + 2x) \\cdot \\sigma(3x + 1) \\)），需将四则运算与链式法则结合使用：\n",
    "- 示例：求 \\( f(x) = (2x + 1) \\cdot \\cos(x^2) \\) 的导数：\n",
    "  1. 拆分为乘积形式：\\( u = 2x + 1 \\)，\\( v = \\cos(x^2) \\)，故 \\( f = uv \\)；\n",
    "  2. 应用乘积法则：\\( f' = u'v + uv' \\)；\n",
    "  3. 对 \\( v = \\cos(x^2) \\) 应用链式法则：\\( v' = -\\sin(x^2) \\cdot 2x \\)；\n",
    "  4. 最终结果：\\( f' = 2 \\cdot \\cos(x^2) + (2x + 1) \\cdot (-\\sin(x^2) \\cdot 2x) = 2\\cos(x^2) - 2x(2x + 1)\\sin(x^2) \\)。\n",
    "\n",
    "### 2. 与高阶导数的关联\n",
    "四则运算可推广到高阶导数，AI 中高阶优化（如牛顿法）需用到二阶导数：\n",
    "- 示例：求 \\( f(x) = x \\cdot e^x \\) 的二阶导数：\n",
    "  1. 一阶导数（乘积法则）：\\( f'(x) = 1 \\cdot e^x + x \\cdot e^x = (1 + x)e^x \\)；\n",
    "  2. 二阶导数（乘积法则）：\\( f''(x) = 1 \\cdot e^x + (1 + x) \\cdot e^x = (2 + x)e^x \\)。\n",
    "\n",
    "### 3. Python 代码验证（实战演示）\n",
    "使用 `sympy` 库验证四则运算法则的正确性，并模拟激活函数与损失函数的梯度计算：\n",
    "\n",
    "```python\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# 1. 符号计算验证四则运算法则\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# 定义两个基础函数（AI 常用）\n",
    "u = x**2 + 3*sp.exp(x)  # u(x) = x² + 3e^x\n",
    "v = sp.sin(x) + sp.ln(x + 1)  # v(x) = sinx + ln(x+1)\n",
    "\n",
    "# 验证和差法则：(u + v)' = u' + v'\n",
    "sum_deriv_manual = sp.diff(u, x) + sp.diff(v, x)\n",
    "sum_deriv_auto = sp.diff(u + v, x)\n",
    "print(\"和差法则验证：\")\n",
    "print(f\"手动计算：{sum_deriv_manual.simplify()}\")\n",
    "print(f\"自动计算：{sum_deriv_auto.simplify()}\")\n",
    "print(f\"是否相等：{sum_deriv_manual.simplify() == sum_deriv_auto.simplify()}\\n\")\n",
    "\n",
    "# 验证乘积法则：(u*v)' = u'v + uv'\n",
    "prod_deriv_manual = sp.diff(u, x)*v + u*sp.diff(v, x)\n",
    "prod_deriv_auto = sp.diff(u*v, x)\n",
    "print(\"乘积法则验证：\")\n",
    "print(f\"手动计算：{prod_deriv_manual.simplify()}\")\n",
    "print(f\"自动计算：{prod_deriv_auto.simplify()}\")\n",
    "print(f\"是否相等：{prod_deriv_manual.simplify() == prod_deriv_auto.simplify()}\\n\")\n",
    "\n",
    "# 验证商法则：(u/v)' = (u'v - uv')/v²\n",
    "quot_deriv_manual = (sp.diff(u, x)*v - u*sp.diff(v, x)) / (v**2)\n",
    "quot_deriv_auto = sp.diff(u/v, x)\n",
    "print(\"商法则验证：\")\n",
    "print(f\"手动计算：{quot_deriv_manual.simplify()}\")\n",
    "print(f\"自动计算：{quot_deriv_auto.simplify()}\")\n",
    "print(f\"是否相等：{quot_deriv_manual.simplify() == quot_deriv_auto.simplify()}\\n\")\n",
    "\n",
    "# 2. 实战：计算 Swish 激活函数的导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def swish_deriv(x):\n",
    "    # 应用乘积法则：(x*σ(x))' = σ(x) + x*σ'(x)\n",
    "    sigma = sigmoid(x)\n",
    "    return sigma + x * sigma * (1 - sigma)\n",
    "\n",
    "# 数值验证\n",
    "x_val = np.array([-1.0, 0.0, 1.0, 2.0])\n",
    "swish_val = swish(x_val)\n",
    "# 用有限差分近似导数（验证理论结果）\n",
    "eps = 1e-6\n",
    "swish_deriv_num = (swish(x_val + eps) - swish(x_val - eps)) / (2*eps)\n",
    "swish_deriv_theo = swish_deriv(x_val)\n",
    "\n",
    "print(\"Swish 函数导数验证（数值近似 vs 理论计算）：\")\n",
    "for x, num, theo in zip(x_val, swish_deriv_num, swish_deriv_theo):\n",
    "    print(f\"x={x}: 数值近似={num:.6f}, 理论计算={theo:.6f}, 误差={abs(num-theo):.6e}\")\n",
    "\n",
    "# 3. 实战：计算 MSE 损失的梯度\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return 0.5 * np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mse_gradient(y_true, y_pred, x):\n",
    "    # MSE 对输入 x 的梯度（假设 y_pred = Wx + b，此处简化为 y_pred = x，仅演示四则运算）\n",
    "    n = len(y_true)\n",
    "    return np.mean((y_pred - y_true) * x)  # 应用和差+乘积法则\n",
    "\n",
    "# 模拟数据\n",
    "y_true = np.array([1.0, 2.0, 3.0])\n",
    "x = np.array([0.5, 1.5, 2.5])\n",
    "y_pred = x  # 简化模型：y_pred = x\n",
    "\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "gradient = mse_gradient(y_true, y_pred, x)\n",
    "print(f\"\\nMSE 损失：{loss:.4f}\")\n",
    "print(f\"MSE 对 x 的梯度：{gradient:.4f}\")\n",
    "```\n",
    "\n",
    "### 4. 学习建议（衔接后续内容）\n",
    "1. **夯实基础**：导数的四则运算是后续链式法则、高阶导数、微分的基础，需熟练掌握“乘积法则不可漏项、商法则符号正确”，建议多练习 AI 常用函数（指数、对数、三角函数与多项式的组合）的求导；\n",
    "2. **关联实战**：每学习一个法则，立即对应到激活函数（如 Swish、ELU）或损失函数（如 MSE、交叉熵）的梯度计算，建立“数学公式→代码实现”的映射；\n",
    "3. **预习衔接**：下一节将学习复合函数求导（链式法则），这是反向传播的核心，建议提前思考“如何将四则运算与链式法则结合”（如 \\( f(x) = (x^2 + e^x) \\cdot \\sin(x^3) \\) 的求导步骤）。"
   ],
   "id": "870818e4d520e429"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
